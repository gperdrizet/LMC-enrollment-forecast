{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d85c20b0-8823-415b-b110-1c12db52a7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # filter out tf warnings - training in a loop causes a retracing efficiency warning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "# Add parent directory to path to allow import of config.py\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import config as conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fe87141-5b9d-49c5-af15-e812f78556d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14 entries, 0 to 13\n",
      "Data columns (total 24 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   Census Enrollment                14 non-null     float64\n",
      " 1   Census Fill Rate                 14 non-null     float64\n",
      " 2   Count of Migrated Data           14 non-null     float64\n",
      " 3   Current Enrollment               14 non-null     float64\n",
      " 4   Current Fill Rate                14 non-null     float64\n",
      " 5   Enrollment Capacity              14 non-null     float64\n",
      " 6   Ftef                             14 non-null     float64\n",
      " 7   Ftes                             14 non-null     float64\n",
      " 8   Ftes/Ftef                        14 non-null     float64\n",
      " 9   Number Retained                  14 non-null     float64\n",
      " 10  Number Successful                14 non-null     float64\n",
      " 11  Retention Rate                   14 non-null     float64\n",
      " 12  Sec. Count                       14 non-null     float64\n",
      " 13  Success Rate                     14 non-null     float64\n",
      " 14  Wsch/Ftef                        14 non-null     float64\n",
      " 15  Next Semester Census Enrollment  14 non-null     float64\n",
      " 16  year_2016                        14 non-null     int64  \n",
      " 17  year_2017                        14 non-null     int64  \n",
      " 18  year_2018                        14 non-null     int64  \n",
      " 19  year_2019                        14 non-null     int64  \n",
      " 20  year_2020                        14 non-null     int64  \n",
      " 21  semester_FA                      14 non-null     int64  \n",
      " 22  semester_SP                      14 non-null     int64  \n",
      " 23  semester_SU                      14 non-null     int64  \n",
      "dtypes: float64(16), int64(8)\n",
      "memory usage: 2.8 KB\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(f'{conf.DATA_PATH}{conf.FORMATTED_DATAFILE}')\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85eb8ef0-d8ad-43e2-b8b5-5413be371d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data randomly into training and testing sets\n",
    "training_data = data.sample(frac=conf.TRAIN_TEST_SPLIT)\n",
    "test_data = data.drop(training_data.index)\n",
    "\n",
    "# Seperate dependent and independent variable\n",
    "training_features = training_data.copy()\n",
    "test_features = test_data.copy()\n",
    "\n",
    "training_labels = training_features.pop(conf.TARGET_VARIABLE)\n",
    "test_labels = test_features.pop(conf.TARGET_VARIABLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "195144fa-02e8-4879-8ee0-63291d47d9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup normalization layer for features\n",
    "normalizer = preprocessing.Normalization()\n",
    "normalizer.adapt(np.array(training_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "650f3d40-ccf2-433c-9d99-5e5e8c4657cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "normalization (Normalization (None, 23)                47        \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                1536      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 5,808\n",
      "Trainable params: 5,761\n",
      "Non-trainable params: 47\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build and compile deep neural network model\n",
    "DNN_model = keras.Sequential([\n",
    "    normalizer,\n",
    "    layers.Dense(conf.UNITS, activation='relu'),\n",
    "    layers.Dense(conf.UNITS, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "DNN_model.compile(\n",
    "    loss='mean_absolute_error',\n",
    "    optimizer=tf.keras.optimizers.Adam(conf.LEARNING_RATE)\n",
    ")\n",
    "\n",
    "DNN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d473a58-b843-4004-b644-c0b3e62a6133",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%time` not found.\n"
     ]
    }
   ],
   "source": [
    "# Do training run\n",
    "%%time\n",
    "\n",
    "history = DNN_model.fit(\n",
    "    training_features, \n",
    "    training_labels,\n",
    "    verbose=0,\n",
    "    epochs=conf.TRAINING_EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b573b5bb-c378-478c-9274-d3c1a8c28495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training data loss during training\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3773de-3eff-4414-a224-90c8f5782337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score model on test data\n",
    "test_score = DNN_model.evaluate(test_features, test_labels, verbose=0)\n",
    "print(f'Test MAE: {int(test_score)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc6ce28-669e-4bdf-beed-87b7b17da6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use model to predict enrollment for test dataset\n",
    "predicted_enrollment = DNN_model.predict(test_features)\n",
    "\n",
    "# Grab actual enrollment numbers to compare with\n",
    "actual_enrollment = test_labels\n",
    "\n",
    "# Plot actual enrollment vs predicted enrollment\n",
    "sns.set(rc={'figure.figsize':(8,8)})\n",
    "\n",
    "ax = sns.regplot(\n",
    "    y=predicted_enrollment, \n",
    "    x=actual_enrollment, \n",
    "    fit_reg=True, \n",
    "    ci=False, \n",
    "    scatter_kws={'s':60}\n",
    ")\n",
    "\n",
    "ax.tick_params(labelsize=12)\n",
    "ax.set_title('Predicted vs actual enrollment', fontsize=18)\n",
    "ax.set_xlabel('Actual enrollment', fontsize=14)\n",
    "ax.set_ylabel('Predicted enrollment', fontsize=15)\n",
    "ax.set_xlim(4000, 27000)\n",
    "ax.set_ylim(4000, 27000)\n",
    "ax.yaxis.set_major_formatter(FormatStrFormatter('%.0f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875a8c48-fe15-46d1-a95c-9029983c4c1a",
   "metadata": {},
   "source": [
    "Looks pretty good! Still very worried about overfitting with such a small dataset. To augment this approach I am going to try something a little unconventional - bootstrap aggregation! This dataset is so small and our network is so simple, training completes on the order of seconds. We will use the speed of training to our advantage and train multiple times with different randomly chosen subsets of the data. Then we will use the ensamble of trained networks to make predictions. It will make more sense when you see it (I hope)...\n",
    "\n",
    "First thing is to encapsulate the major opperations above into functions so we can easily build and train models in a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60ffa2c-a2ab-444a-9a6e-e583e898e40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(data, training_data_fraction, label_column_name):\n",
    "    '''Takes master data as pandas dataframe splits into train and\n",
    "    test features and labels, returns a dict of dataframes'''\n",
    "    \n",
    "    # Split data randomly into training and testing sets\n",
    "    training_data = data.sample(frac=training_data_fraction)\n",
    "    test_data = data.drop(training_data.index)\n",
    "\n",
    "    # Seperate dependent and independent variable\n",
    "    training_features = training_data.copy()\n",
    "    test_features = test_data.copy()\n",
    "\n",
    "    training_labels = training_features.pop(label_column_name)\n",
    "    test_labels = test_features.pop(label_column_name)\n",
    "    \n",
    "    dataset = {}\n",
    "    dataset['training_features'] = training_features\n",
    "    dataset['test_features'] = test_features\n",
    "    dataset['training_labels'] = training_labels\n",
    "    dataset['test_labels'] = test_labels\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6ffca5-b5f9-46e7-9d24-17c9d0ac8fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_normalization_layer(training_features):\n",
    "    '''Takes a pandas dataframe, returns an adapted tf.keras\n",
    "    normalization layer'''\n",
    "    \n",
    "    normalizer = preprocessing.Normalization()\n",
    "    normalizer.adapt(np.array(training_features))\n",
    "    \n",
    "    return normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d99cbad-4cf5-430e-81b2-8077a6e212e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_compile_model(normalizer, units, learning_rate):\n",
    "    '''Take pre-adapted normalization layer and hyperparameters,\n",
    "    return compiled neural network model'''\n",
    "    \n",
    "    DNN_model = keras.Sequential([\n",
    "        normalizer,\n",
    "        layers.Dense(units, activation='relu'),\n",
    "        layers.Dense(units, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    DNN_model.compile(\n",
    "        loss='mean_absolute_error',\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate)\n",
    "    )\n",
    "    \n",
    "    return DNN_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e91ce2-814b-42ed-94bf-9cc131b18a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First make some empty lists to hold our history objects \n",
    "# and trained models\n",
    "\n",
    "trained_DNN_models = []\n",
    "training_history_objects = []\n",
    "test_set_scores = []\n",
    "\n",
    "# Outer training loop\n",
    "for i in range(conf.N_MODELS):\n",
    "    # Make dataset\n",
    "    dataset = create_datasets(\n",
    "        data,\n",
    "        conf.TRAIN_TEST_SPLIT,\n",
    "        conf.TARGET_VARIABLE\n",
    "    )\n",
    "    \n",
    "    # Adapt normalization layer\n",
    "    normalizer = create_normalization_layer(dataset['training_features'])\n",
    "    \n",
    "    # Build model\n",
    "    dnn_model = build_and_compile_model(\n",
    "        normalizer,\n",
    "        conf.UNITS,\n",
    "        conf.LEARNING_RATE\n",
    "    )\n",
    "    \n",
    "    print(f'Training model {i}')\n",
    "    \n",
    "    # Train model\n",
    "    history = dnn_model.fit(\n",
    "        dataset['training_features'], \n",
    "        dataset['training_labels'],\n",
    "        verbose=0,\n",
    "        epochs=conf.TRAINING_EPOCHS\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    trained_DNN_models.append(dnn_model)\n",
    "    training_history_objects.append(history)\n",
    "    \n",
    "    # Evaluate model\n",
    "    test_score = dnn_model.evaluate(\n",
    "        dataset['test_features'], \n",
    "        dataset['test_labels'],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    test_set_scores.append(test_score)\n",
    "    \n",
    "    print(f'Model {i} test MAE: {test_score}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40832e50-0e2a-49d3-be65-8ac412c46376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of MAE scores from each model\n",
    "sns.displot(test_set_scores, kind=\"kde\")\n",
    "plt.tick_params(labelsize=12)\n",
    "plt.xlabel('Mean absolute error', fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7ebccb-60e5-4b69-9bc0-134a558a5fcd",
   "metadata": {},
   "source": [
    "OK, still higher than I'd like - but much much better than the linear bagging model. There the error distribution had it's major peak around 10000 - and that was for the whole dataset. This distribution is true test set error for each model.\n",
    "\n",
    "Let's try and use to to make some predictions. Strategy here will be to loop through the list of trained models, use each one to make a list of predictions and then average them at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7baaa7a-1d5f-4fd8-ace2-2ad88038f8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split master data into features and labels\n",
    "X = data.drop('Next Semester Census Enrollment', axis=1)\n",
    "y = data['Next Semester Census Enrollment']\n",
    "\n",
    "# Empty list to hold predictions\n",
    "predictions = []\n",
    "\n",
    "for DNN_model in trained_DNN_models:\n",
    "    predicted_enrollment = DNN_model.predict(X)\n",
    "    predictions.append(predicted_enrollment)\n",
    "\n",
    "predicted_enrollment = np.mean(np.array(predictions), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31685dc8-42c4-4079-b790-470ad3aa45f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab actual enrollment numbers to compare with\n",
    "actual_enrollment = y\n",
    "\n",
    "# Plot actual enrollment vs predicted enrollment\n",
    "sns.set(rc={'figure.figsize':(8,8)})\n",
    "\n",
    "ax = sns.regplot(\n",
    "    y=predicted_enrollment, \n",
    "    x=actual_enrollment, \n",
    "    fit_reg=True, \n",
    "    ci=False, \n",
    "    scatter_kws={'s':60}\n",
    ")\n",
    "\n",
    "ax.tick_params(labelsize=12)\n",
    "ax.set_title('Predicted vs actual enrollment', fontsize=18)\n",
    "ax.set_xlabel('Actual enrollment', fontsize=14)\n",
    "ax.set_ylabel('Predicted enrollment', fontsize=15)\n",
    "ax.set_xlim(4000, 27000)\n",
    "ax.set_ylim(4000, 27000)\n",
    "ax.yaxis.set_major_formatter(FormatStrFormatter('%.0f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae33992c-3c10-4edf-ad29-be603b81d8b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
